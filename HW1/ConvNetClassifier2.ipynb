{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors, GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TEXT = torchtext.data.Field()\n",
    "LABEL = torchtext.data.Field(sequential=False)\n",
    "train, val, test = torchtext.datasets.SST.splits(\n",
    "    TEXT, LABEL,\n",
    "    filter_pred=lambda ex: ex.label != 'neutral')\n",
    "\n",
    "TEXT.build_vocab(train)\n",
    "LABEL.build_vocab(train)\n",
    "n_vocab = len(TEXT.vocab)\n",
    "\n",
    "url = 'https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.simple.vec'\n",
    "TEXT.vocab.load_vectors(vectors=Vectors('wiki.simple.vec', url=url))\n",
    "n_comps = TEXT.vocab.vectors.size(1)\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "train_iter, val_iter, test_iter = torchtext.data.BucketIterator.splits(\n",
    "    (train, val, test), batch_size=BATCH_SIZE, device=-1, repeat=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ConvNetClassifier(nn.Module):\n",
    "    \n",
    "    def __init__(self, vecs, dropout_rate=0.5):\n",
    "        super(ConvNetClassifier, self).__init__()\n",
    "        self.vecs = vecs\n",
    "        self.conv3 = nn.Conv2d(in_channels=1, out_channels=100, kernel_size=(3, n_comps))\n",
    "        self.conv4 = nn.Conv2d(in_channels=1, out_channels=100, kernel_size=(4, n_comps))\n",
    "        self.conv5 = nn.Conv2d(in_channels=1, out_channels=100, kernel_size=(5, n_comps))\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.dropout = nn.Dropout2d(p=dropout_rate)\n",
    "        self.linear = nn.Linear(300, 1)\n",
    "    \n",
    "    def forward(self, text, training=False):\n",
    "        while text.size(0) < 5:\n",
    "            text = torch.cat([text, torch.ones((1, text.size(1))).long()], 0)\n",
    "        sent_length, batch_size = text.size()\n",
    "        X = self.vecs[text.data.view(-1,)].view(sent_length, batch_size, n_comps)\n",
    "        X = X.permute(1, 0, 2)\n",
    "        X = X.data.unsqueeze_(1)\n",
    "        X = Variable(X)\n",
    "        \n",
    "        # Extract and pool convolutional features\n",
    "        X3 = F.relu(self.conv3(X))\n",
    "        X3 = F.max_pool2d(X3, (X3.size(2), 1))\n",
    "        X4 = F.relu(self.conv4(X))\n",
    "        X4 = F.max_pool2d(X4, (X4.size(2), 1))\n",
    "        X5 = F.relu(self.conv5(X))\n",
    "        X5 = F.max_pool2d(X5, (X5.size(2), 1))\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        if training:\n",
    "            X3 = self.dropout(X3)\n",
    "            X4 = self.dropout(X4)\n",
    "            X5 = self.dropout(X5) \n",
    "        \n",
    "        # Final layer\n",
    "        X = torch.cat([X3, X4, X5], 1).squeeze()\n",
    "        probs = F.sigmoid(self.linear(X))\n",
    "        return torch.cat([probs, 1-probs], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class NaiveBayesClassifier:\n",
    "    def __init__(self, alpha, beta, n_features):\n",
    "        # 1 x C vector; dirichlet prior for class distr.\n",
    "        # C = 2 for binary classification.\n",
    "        self.alpha = alpha\n",
    "        self.alpha0 = sum(alpha)\n",
    "\n",
    "        # 1 x K vector; dirichlet prior for class conditional distr.\n",
    "        # K = 2 for binary features, otherwise K = max(occurences_of_word_in_text)\n",
    "        self.beta = beta\n",
    "        self.beta0 = sum(beta)\n",
    "\n",
    "        # dimensions of data\n",
    "        self.C = len(self.alpha) # num classes\n",
    "        self.K = len(self.beta)  # num possible values for each feature (count)\n",
    "        self.D = n_features      # num features (size of vocabulary)\n",
    "\n",
    "        # counts\n",
    "        self.N = 0\n",
    "        self.N_c = np.zeros(self.C, dtype=int)\n",
    "        self.N_cj = np.zeros((self.C, self.D), dtype=int)\n",
    "        self.N_ckj = np.zeros((self.C, self.K, self.D), dtype=int)\n",
    "\n",
    "        self.flushed = False\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = X.astype(int)\n",
    "        N, _D = X.shape\n",
    "        self.N += N\n",
    "\n",
    "        # print(\"Fitting model\")\n",
    "        for c in range(self.C):\n",
    "            msk = y == c\n",
    "            self.N_c[c] += np.sum(msk)\n",
    "            self.N_cj[c] += np.sum(X[msk], dtype=int, axis=0)\n",
    "            self.N_ckj[c] += np.apply_along_axis(np.bincount, 0, X[msk], minlength=self.K)\n",
    "\n",
    "        self.flushed = False\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = X.astype(int)\n",
    "\n",
    "        if not self.flushed:\n",
    "            # print(\"Flushing\")\n",
    "            self.pi = np.array([ # class distribution\n",
    "                np.log(self.N_c[c] + self.alpha[c]) - np.log(self.N + self.alpha0)\n",
    "                for c in range(self.C)])\n",
    "            self.mu = np.fromfunction( # log prob of each (class, count, word) tuple\n",
    "                lambda c, j, k: np.log(self.N_ckj[c, k, j] + self.beta[c]) - np.log(self.N_c[c] + self.beta0),\n",
    "                (self.C, self.D, self.K), dtype=int)\n",
    "            self.flushed = True\n",
    "\n",
    "        # print(\"Predicting labels\")\n",
    "        p_for_x = lambda x: [ # calculate log probability for x of class c\n",
    "            self.pi[c] + np.sum([self.mu[c, j, x[j]] for j in range(len(x))])\n",
    "            for c in range(self.C)]\n",
    "        ps = np.apply_along_axis(p_for_x, 1, X)\n",
    "        return ps # get predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bag_of_words(batch, TEXT):\n",
    "    \"\"\"\n",
    "    returns bag of words representation (Variable) of a batch.\n",
    "    each bag of words has dimension [batch_size, vocab_size].\n",
    "    \"\"\"\n",
    "    V = len(TEXT.vocab)\n",
    "    X = torch.zeros(batch.text.size(0), V)\n",
    "    ones = torch.ones(batch.text.size(1))\n",
    "    for b in range(batch.text.size(0)):\n",
    "        X[b].index_add_(0, batch.text.data[b], ones)\n",
    "        X[b][TEXT.vocab.stoi['<pad>']] = 0\n",
    "    X = Variable(X, requires_grad=False)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alpha = a * np.ones(C)\n",
    "beta = b * np.ones(K)\n",
    "n_features = len(TEXT.vocab)\n",
    "nb = NaiveBayesClassifier(alpha, beta, n_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = bag_of_words(batch, TEXT).data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i, batch in enumerate(train_iter):\n",
    "    batch.text = batch.text.transpose(1, 0)\n",
    "    X = bag_of_words(batch, TEXT).data.numpy()\n",
    "    if binary:\n",
    "        X = X > 0\n",
    "    y = batch.label.data.numpy() - 1\n",
    "    nb.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n, n_corr = 0, 0\n",
    "for i, batch in enumerate(test_iter):\n",
    "    probs = cn(batch.text).data.numpy()\n",
    "    y_pred = probs.argmax(1)\n",
    "    batch.text = batch.text.transpose(1, 0)\n",
    "    X = bag_of_words(batch, TEXT).data.numpy()\n",
    "    if binary:\n",
    "        X = X > 0\n",
    "    probs2 = softmax(nb.predict(X))\n",
    "    y_pred = (probs + probs2).argmax(1)\n",
    "    y = batch.label.data.numpy() - 1\n",
    "\n",
    "    n += len(y)\n",
    "    n_corr += sum(y_pred == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79406919275123555"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_corr / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration #0: 0.6453072428703308\n",
      "Iteration #1: 0.5488080382347107\n",
      "Iteration #2: 0.5789662599563599\n",
      "Iteration #3: 0.5029851198196411\n",
      "Iteration #4: 0.4807746112346649\n",
      "Iteration #5: 0.4014658033847809\n",
      "Iteration #6: 0.3421747088432312\n",
      "Iteration #7: 0.29946184158325195\n",
      "Iteration #8: 0.41238003969192505\n",
      "Iteration #9: 0.3111019432544708\n",
      "Iteration #10: 0.24287666380405426\n",
      "Iteration #11: 0.19000859558582306\n",
      "Iteration #12: 0.3005876839160919\n",
      "Iteration #13: 0.16436639428138733\n",
      "Iteration #14: 0.1309184730052948\n",
      "Iteration #15: 0.15655812621116638\n",
      "Iteration #16: 0.18515485525131226\n",
      "Iteration #17: 0.27649080753326416\n",
      "Iteration #18: 0.14792419970035553\n",
      "Iteration #19: 0.10025647282600403\n"
     ]
    }
   ],
   "source": [
    "vecs = Variable(TEXT.vocab.vectors, requires_grad=True)\n",
    "cn = ConvNetClassifier(vecs)\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.Adam(cn.parameters(), lr=0.0003)\n",
    "optimizer2 = optim.Adam([cn.vecs], lr=0.0001)\n",
    "#optimizer = optim.SGD(cn.parameters(), lr=0.03, weight_decay=0.01)\n",
    "#optimizer = optim.Adadelta(cn.parameters(), lr=0.1)\n",
    "max_vec_size = 3\n",
    "\n",
    "for i in range(20):\n",
    "    train_iter.init_epoch()\n",
    "    for batch in train_iter:\n",
    "        cn.zero_grad()\n",
    "        probs = cn(batch.text, training=True)\n",
    "        log_probs = torch.log(probs)\n",
    "        y = batch.label - 1\n",
    "        loss = loss_function(log_probs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "#         optimizer2.step()\n",
    "        \n",
    "        # Regularization\n",
    "#         for w in cn.parameters():\n",
    "#             w_2norm = w.data.norm(2)\n",
    "#             if w_2norm > max_vec_size:\n",
    "#                 w.data = max_vec_size / w_2norm * w.data\n",
    "    print('Iteration #{}: {}'.format(i, loss.data.numpy()[0]))\n",
    "cn.linear.weight.data *= 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lengths = []\n",
    "train_iter.init_epoch()\n",
    "for batch in train_iter:\n",
    "    lengths.append(batch.text.size(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "52"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def evaluate(model, data_iter):\n",
    "    data_iter.init_epoch()\n",
    "    N = len(data_iter.data())\n",
    "    n_correct = 0\n",
    "    data_iter.init_epoch()\n",
    "    for batch in data_iter:\n",
    "        probs = model(batch.text)\n",
    "        _, y_predicted = probs.max(1)\n",
    "        y_true = batch.label - 1\n",
    "        n_correct += (y_true == y_predicted).sum().float()\n",
    "    return (n_correct / N).data.numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99436414"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(cn, train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.79187262"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(cn, test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.999999962180317"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn.linear.weight.data.norm(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_linear.weight.data = test_linear.weight.data * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4999999810901585"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn.linear.weight.data.norm(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "\n",
       "Columns 0 to 9 \n",
       " 0.0919  0.0995 -0.0591 -0.0713 -0.0962 -0.0985 -0.0943 -0.0954  0.0753 -0.0675\n",
       "\n",
       "Columns 10 to 19 \n",
       "-0.0731 -0.0407 -0.0846  0.0974 -0.0862 -0.0913  0.0820 -0.0800  0.0970  0.0920\n",
       "\n",
       "Columns 20 to 29 \n",
       " 0.0867  0.0855 -0.0874  0.1046  0.0522  0.1021 -0.0905 -0.0797  0.0556 -0.0818\n",
       "\n",
       "Columns 30 to 39 \n",
       "-0.0948 -0.0248  0.0984  0.0363 -0.0818 -0.0781 -0.0681  0.0765 -0.0971  0.0563\n",
       "\n",
       "Columns 40 to 49 \n",
       "-0.0753 -0.1091 -0.0936 -0.0554 -0.0742 -0.0950 -0.1096  0.0906 -0.0903 -0.0933\n",
       "\n",
       "Columns 50 to 59 \n",
       "-0.0866 -0.0796 -0.0287 -0.0763 -0.1002 -0.0852 -0.0730 -0.0770 -0.1007  0.0883\n",
       "\n",
       "Columns 60 to 69 \n",
       "-0.0883 -0.1000 -0.0548 -0.0860  0.0809 -0.0816  0.0910 -0.0962  0.0917  0.0973\n",
       "\n",
       "Columns 70 to 79 \n",
       "-0.0786  0.1067  0.0942  0.0807 -0.0860 -0.0528  0.0952  0.0870  0.0752 -0.0895\n",
       "\n",
       "Columns 80 to 89 \n",
       "-0.1118  0.0762  0.0893 -0.0818  0.0850 -0.1047 -0.1015  0.0950  0.0759 -0.0389\n",
       "\n",
       "Columns 90 to 99 \n",
       "-0.0879 -0.0937  0.0425 -0.0923 -0.0821 -0.0937  0.0584 -0.0809  0.0729  0.0951\n",
       "\n",
       "Columns 100 to 109 \n",
       " 0.1009  0.0434  0.1032  0.1031  0.0406  0.0810  0.0463  0.0994 -0.0863 -0.0649\n",
       "\n",
       "Columns 110 to 119 \n",
       " 0.0838 -0.0963  0.0904 -0.0873 -0.0883 -0.0983 -0.1001 -0.0943  0.0742 -0.1022\n",
       "\n",
       "Columns 120 to 129 \n",
       " 0.1043 -0.0919  0.0987 -0.0922  0.0425  0.0977  0.0836 -0.0995 -0.1021  0.0655\n",
       "\n",
       "Columns 130 to 139 \n",
       "-0.0943  0.0890  0.0972 -0.0934  0.1019  0.0910  0.0870  0.0901 -0.0552  0.0632\n",
       "\n",
       "Columns 140 to 149 \n",
       "-0.0983 -0.0920 -0.1028 -0.0535  0.0921 -0.0871 -0.1036  0.0701 -0.0725 -0.0771\n",
       "\n",
       "Columns 150 to 159 \n",
       " 0.0866  0.0847 -0.0785  0.0738  0.0834  0.1103 -0.0945  0.0867  0.0832 -0.0960\n",
       "\n",
       "Columns 160 to 169 \n",
       "-0.1006 -0.0687  0.0942  0.0906  0.0793 -0.0861 -0.1050 -0.1055 -0.0771  0.0821\n",
       "\n",
       "Columns 170 to 179 \n",
       "-0.1031  0.0882 -0.0847 -0.1069 -0.0836 -0.0421  0.1019  0.0839  0.0647  0.0081\n",
       "\n",
       "Columns 180 to 189 \n",
       "-0.0956 -0.0720  0.1049  0.0945  0.0856 -0.0892 -0.1035  0.0582  0.0769 -0.0835\n",
       "\n",
       "Columns 190 to 199 \n",
       " 0.0753 -0.0867  0.0592  0.0980 -0.0409 -0.0832  0.0836  0.0753  0.1044 -0.0984\n",
       "\n",
       "Columns 200 to 209 \n",
       " 0.0962 -0.0877  0.0968  0.0862 -0.0942 -0.1021 -0.0914  0.0893 -0.0820 -0.1046\n",
       "\n",
       "Columns 210 to 219 \n",
       " 0.0955 -0.0604  0.0620 -0.1077 -0.1064  0.0711  0.0904  0.0653 -0.0998  0.1007\n",
       "\n",
       "Columns 220 to 229 \n",
       "-0.1003  0.0681 -0.0985 -0.0557 -0.0882  0.0820 -0.0721 -0.0328  0.0917  0.0618\n",
       "\n",
       "Columns 230 to 239 \n",
       " 0.0944 -0.1070  0.0823  0.0744 -0.1107  0.0865 -0.0938  0.0878 -0.1064  0.0903\n",
       "\n",
       "Columns 240 to 249 \n",
       " 0.0836 -0.0854 -0.0698  0.0773  0.1079  0.1030  0.0835 -0.0589 -0.0946  0.0931\n",
       "\n",
       "Columns 250 to 259 \n",
       " 0.0745  0.0960  0.0870 -0.0841 -0.0886  0.0692 -0.0923  0.0963  0.1012  0.0867\n",
       "\n",
       "Columns 260 to 269 \n",
       " 0.0924  0.1071 -0.0491  0.0796 -0.0439 -0.1018  0.0857  0.1013  0.0940  0.0934\n",
       "\n",
       "Columns 270 to 279 \n",
       "-0.0955  0.0983  0.0992  0.0993  0.0863  0.0977 -0.1029  0.0944  0.0838 -0.0993\n",
       "\n",
       "Columns 280 to 289 \n",
       " 0.0956 -0.0939 -0.0886  0.0667 -0.0988 -0.0767  0.0654  0.0906 -0.0900  0.0935\n",
       "\n",
       "Columns 290 to 299 \n",
       " 0.0747  0.0918 -0.0724  0.1002  0.1058  0.1067  0.0762 -0.0973 -0.0466  0.0732\n",
       "[torch.FloatTensor of size 1x300]"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = cn.linear.weight * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_vec_size = 3\n",
    "w_2norm = cn.linear.weight.data.norm(2)\n",
    "cn.linear.weight.data = max_vec_size / w_2norm * cn.linear.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot assign 'torch.autograd.variable.Variable' as parameter 'weight' (torch.nn.Parameter or None expected)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-129-8ab163ff9165>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m//anaconda/envs/py35/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__setattr__\u001b[0;34m(self, name, value)\u001b[0m\n\u001b[1;32m    279\u001b[0m                 raise TypeError(\"cannot assign '{}' as parameter '{}' \"\n\u001b[1;32m    280\u001b[0m                                 \u001b[0;34m\"(torch.nn.Parameter or None expected)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m                                 .format(torch.typename(value), name))\n\u001b[0m\u001b[1;32m    282\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister_parameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot assign 'torch.autograd.variable.Variable' as parameter 'weight' (torch.nn.Parameter or None expected)"
     ]
    }
   ],
   "source": [
    "cn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = f.weight * 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "bool value of Variable objects containing non-empty torch.ByteTensor is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-97-338bb810f20c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_predicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mn_correct\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my_predicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m//anaconda/envs/py35/lib/python3.5/site-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36m__bool__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         raise RuntimeError(\"bool value of Variable objects containing non-empty \" +\n\u001b[0;32m--> 123\u001b[0;31m                            torch.typename(self.data) + \" is ambiguous\")\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     \u001b[0m__nonzero__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__bool__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: bool value of Variable objects containing non-empty torch.ByteTensor is ambiguous"
     ]
    }
   ],
   "source": [
    "test_iter.init_epoch()\n",
    "N = len(test_iter.data())\n",
    "n_correct = 0\n",
    "test_iter.init_epoch()\n",
    "for batch in test_iter:\n",
    "    probs = cn(batch.text)\n",
    "    _, y_predicted = probs.max(1)\n",
    "    y_true = batch.label - 1\n",
    "    if (y_true == y_predicted).sum() != 50:\n",
    "        raise ValueError()\n",
    "    n_correct += (y_true == y_predicted).sum().float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.3969\n",
       " 0.4813\n",
       " 0.5928\n",
       " 0.4208\n",
       " 0.4645\n",
       " 0.2857\n",
       "[torch.FloatTensor of size 6]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs[:, 0][~(y_true == y_predicted).data]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
