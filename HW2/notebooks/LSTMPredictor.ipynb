{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchtext\n",
    "from torchtext.vocab import Vectors\n",
    "from collections import deque\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10db17390>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TEXT = torchtext.data.Field()\n",
    "train, val, test = torchtext.datasets.LanguageModelingDataset.splits(\n",
    "    path=\"\", train=\"train.txt\", validation=\"valid.txt\", test=\"valid.txt\", text_field=TEXT)\n",
    "TEXT.build_vocab(train)\n",
    "train_iter, val_iter, test_iter = torchtext.data.BPTTIterator.splits(\n",
    "    (train, val, test), batch_size=20, device=-1, bptt_len=35, repeat=False)\n",
    "# Build the vocabulary with word embeddings\n",
    "url = 'https://s3-us-west-1.amazonaws.com/fasttext-vectors/wiki.simple.vec'\n",
    "TEXT.vocab.load_vectors(vectors=Vectors('wiki.simple.vec', url=url))\n",
    "\n",
    "it = iter(train_iter)\n",
    "batch = next(it) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       " 0.0104 -0.1829  0.0761  ...  -0.1362 -0.2240 -0.0552\n",
       "          ...             ⋱             ...          \n",
       " 0.0000  0.0000  0.0000  ...   0.0000  0.0000  0.0000\n",
       "-0.0421  0.0775  0.1566  ...   0.0535  0.3011 -0.1736\n",
       " 0.0004 -0.2599  0.1088  ...   0.0129  0.2687 -0.1937\n",
       "[torch.FloatTensor of size 10001x300]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TEXT.vocab.vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM_Lang_Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, embeddings, dropout_rate):\n",
    "        super(LSTM_Lang_Model, self).__init__()\n",
    "        self.embeddings = embeddings\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.lstm = nn.LSTM(input_size=embeddings.size(1), hidden_size=650, num_layers=2, dropout=dropout_rate)\n",
    "        for w in rnn.lstm.parameters():\n",
    "            w.data = torch.Tensor(w.size()).uniform_(-0.05, 0.05)\n",
    "        self.dropout_o = nn.Dropout(p=dropout_rate)\n",
    "        self.linear = nn.Linear(650, embeddings.size(0))\n",
    "    \n",
    "    def forward(self, text, hidden):\n",
    "        input = Variable(self.embeddings[batch.text.data, :])\n",
    "        output, hidden = self.lstm(input, hidden)\n",
    "        self.output = output\n",
    "        output = self.dropout_o(output)\n",
    "        output = self.linear(output)\n",
    "        probs = F.softmax(output.permute(2, 0, 1)).permute(1, 2, 0)\n",
    "        return probs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "rnn = LSTM_Lang_Model(embeddings=TEXT.vocab.vectors, dropout_rate=0.5)\n",
    "optimizer = optim.SGD(rnn.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.21388\n",
      "9.21179\n",
      "9.20935\n",
      "9.20507\n",
      "9.19739\n",
      "9.18771\n"
     ]
    }
   ],
   "source": [
    "train_iter.init_epoch()\n",
    "hidden = None\n",
    "for batch in train_iter:\n",
    "    probs, hidden = rnn(batch.text, hidden)\n",
    "    log_probs = torch.log(probs)\n",
    "    loss = -log_probs.gather(2, batch.target.unsqueeze(2)).mean()\n",
    "    loss.backward(retain_graph=True)\n",
    "    for w in rnn.lstm.parameters():\n",
    "        norm = w.grad.norm()\n",
    "        if norm.data.numpy()[0] > 5:\n",
    "            print('yes')\n",
    "            w.grad = w.grad / norm\n",
    "    optimizer.step()\n",
    "    print(loss.data.numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Variable containing:\n",
       " ( 0 ,.,.) = \n",
       "   2.3730e-02 -4.9902e-02 -4.3526e-02  ...   6.5671e-03  1.6821e-02  2.7558e-03\n",
       "  -3.7160e-02 -1.7295e-02 -3.4130e-02  ...  -1.3602e-02  3.8968e-03 -1.4136e-02\n",
       "   2.1907e-02 -3.9972e-02 -1.7203e-02  ...   8.0944e-04 -2.6492e-02  1.3127e-03\n",
       "                  ...                   ⋱                   ...                \n",
       "   2.2806e-02 -7.0939e-03 -3.9586e-02  ...  -2.0796e-02  7.2639e-03  4.8472e-03\n",
       "   3.1893e-04 -4.3660e-02 -3.8795e-02  ...  -2.1925e-02  5.0662e-03 -4.0616e-03\n",
       "  -3.7838e-03 -2.4350e-02 -6.6424e-02  ...  -8.2639e-03  3.2982e-02 -1.5917e-02\n",
       " \n",
       " ( 1 ,.,.) = \n",
       "  -1.8233e-02 -4.4981e-02  7.2307e-02  ...  -2.2500e-02  5.3280e-02 -5.3389e-02\n",
       "  -2.4832e-02 -4.3759e-02  5.7221e-02  ...  -2.5631e-02  4.7603e-02 -4.6040e-02\n",
       "  -2.5052e-02 -5.5347e-02  6.3033e-02  ...  -1.5348e-02  4.5850e-02 -4.5402e-02\n",
       "                  ...                   ⋱                   ...                \n",
       "  -3.0527e-02 -3.8360e-02  5.8920e-02  ...  -2.0339e-02  4.3557e-02 -5.1226e-02\n",
       "  -1.9795e-02 -4.5994e-02  7.1272e-02  ...  -1.7908e-02  3.6602e-02 -5.1424e-02\n",
       "  -1.3503e-02 -4.0880e-02  6.7385e-02  ...  -2.7305e-02  4.2575e-02 -5.6984e-02\n",
       " [torch.FloatTensor of size 2x10x650], Variable containing:\n",
       " ( 0 ,.,.) = \n",
       "   0.0464 -0.1032 -0.0948  ...   0.0136  0.0341  0.0056\n",
       "  -0.0751 -0.0344 -0.0695  ...  -0.0285  0.0083 -0.0299\n",
       "   0.0443 -0.0801 -0.0326  ...   0.0017 -0.0528  0.0028\n",
       "            ...             ⋱             ...          \n",
       "   0.0441 -0.0148 -0.0752  ...  -0.0422  0.0149  0.0096\n",
       "   0.0006 -0.0841 -0.0788  ...  -0.0442  0.0102 -0.0081\n",
       "  -0.0074 -0.0484 -0.1288  ...  -0.0170  0.0654 -0.0307\n",
       " \n",
       " ( 1 ,.,.) = \n",
       "  -0.0368 -0.0905  0.1475  ...  -0.0448  0.1066 -0.1043\n",
       "  -0.0495 -0.0924  0.1133  ...  -0.0508  0.0932 -0.0900\n",
       "  -0.0505 -0.1108  0.1261  ...  -0.0316  0.0920 -0.0919\n",
       "            ...             ⋱             ...          \n",
       "  -0.0618 -0.0798  0.1163  ...  -0.0402  0.0849 -0.0980\n",
       "  -0.0397 -0.0950  0.1429  ...  -0.0367  0.0713 -0.1022\n",
       "  -0.0269 -0.0862  0.1370  ...  -0.0553  0.0824 -0.1122\n",
       " [torch.FloatTensor of size 2x10x650])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [py35]",
   "language": "python",
   "name": "Python [py35]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
