{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HW 4: Variational Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this homework you will build a deep generative model of binary images (MNIST) using variational autoencoders and generative adversarial networks.\n",
    "The original VAE paper can be found [here](https://arxiv.org/abs/1312.6114) and GANs [here](https://arxiv.org/abs/1406.2661), and there are many excellent tutorials\n",
    "online, e.g. [here](https://arxiv.org/abs/1606.05908) and [here](https://jaan.io/what-is-variational-autoencoder-vae-tutorial/)\n",
    "\n",
    "**For this homework there will not be a Kaggle submission**\n",
    "\n",
    "## Goals\n",
    "\n",
    "\n",
    "1. Build a discrete deep generative model of binary digits (MNIST) using variational autoencoders\n",
    "2. Examine the learned latent space with visualizations \n",
    "3. Build a continuous deep generative model using generative adversarial networks.\n",
    "4. Additionally extend the above in any way, for example by :\n",
    "    - using better encoder/decoders (e.g. CNN as the encoder, PixelCNN as the decoder. Description of PixelCNN \n",
    "    can be found [here](https://arxiv.org/abs/1601.06759))\n",
    "    - using different variational families, e.g. with [normalizing flows](https://arxiv.org/abs/1505.05770), \n",
    "    [inverse autoregressive flows](https://arxiv.org/pdf/1606.04934.pdf), \n",
    "    [hierarchical models](https://arxiv.org/pdf/1602.02282.pdf)\n",
    "    - comparing with stochastic variational inference (i.e. where your variational parameters are randomly initialized and\n",
    "    then updated with gradient ascent on the ELBO\n",
    "    - or your own extension.\n",
    "\n",
    "For your encoder/decoder, we suggest starting off with simple models (e.g. 2-layer MLP with ReLUs).\n",
    "\n",
    "Consult the papers provided for hyperparameters, and the course notes for formal definitions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "This notebook provides a working definition of the setup of the problem itself. Feel free to construct your models inline, or use an external setup (preferred) to build your system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, as always, let's download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# The output of torchvision datasets are PILImage images of range [0, 1].\n",
    "# We transform them to Tensors of normalized range [-1, 1]\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
    "    ])\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data/',\n",
    "                            train=True, \n",
    "                            transform=transform,\n",
    "                            download=True)\n",
    "test_dataset = datasets.MNIST(root='./data/',\n",
    "                           train=False, \n",
    "                           transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_dataset))\n",
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default MNIST gives grayscale values between [0,1]. Since we are modeling binary images, we have to turn these\n",
    "into binary values, i.e. $\\{0,1\\}^{784}$). A standard way to do this is to interpret the grayscale values as \n",
    "probabilities and sample Bernoulli random vectors based on these probabilities. (Note you should not do this for GANs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 1, 28, 28]) torch.Size([60000]) torch.Size([10000, 1, 28, 28]) torch.Size([10000])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(3435)\n",
    "train_img = torch.stack([d[0] for d in train_dataset])\n",
    "train_label = torch.LongTensor([d[1] for d in train_dataset])\n",
    "test_img = torch.stack([d[0] for d in test_dataset])\n",
    "test_label = torch.LongTensor([d[1] for d in test_dataset])\n",
    "print(train_img.size(), train_label.size(), test_img.size(), test_label.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST does not have an official train dataset. So we will use the last 10000 training points as your validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "val_img = train_img[-10000:].clone()\n",
    "val_label = train_label[-10000:].clone()\n",
    "train_img = train_img[:-10000]\n",
    "train_label = train_label[:-10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the dataloader to split into batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = torch.utils.data.TensorDataset(train_img, train_label)\n",
    "val = torch.utils.data.TensorDataset(val_img, val_label)\n",
    "test = torch.utils.data.TensorDataset(test_img, test_label)\n",
    "\n",
    "BATCH_SIZE = 100\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_var(x):\n",
    "    if USE_CUDA:\n",
    "        x = x.cuda()\n",
    "    return Variable(x)\n",
    "\n",
    "def denorm(x):\n",
    "    out = (x + 1) / 2\n",
    "    return out.clamp(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LATENT_SIZE = 64\n",
    "H_DIM = 256\n",
    "OUT_DIM = 784\n",
    "\n",
    "# Discriminator\n",
    "_D = nn.Sequential(\n",
    "    nn.Linear(OUT_DIM, H_DIM),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(H_DIM, H_DIM),\n",
    "    nn.LeakyReLU(0.2),\n",
    ")\n",
    "\n",
    "Dz = nn.Sequential(\n",
    "    _D,\n",
    "    nn.Linear(H_DIM, 1),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "Dy = nn.Sequential(\n",
    "    _D,\n",
    "    nn.Linear(H_DIM, 10),\n",
    "    nn.Softmax()\n",
    ")\n",
    "\n",
    "# Generator \n",
    "G = nn.Sequential(\n",
    "    nn.Linear(LATENT_SIZE, H_DIM),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(H_DIM, H_DIM),\n",
    "    nn.LeakyReLU(0.2),\n",
    "    nn.Linear(H_DIM, OUT_DIM),\n",
    "    nn.Tanh()\n",
    ")\n",
    "\n",
    "if USE_CUDA:\n",
    "    D.cuda()\n",
    "    G.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Binary cross entropy loss and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "d_optimizer = torch.optim.Adam(D.parameters(), lr=0.0003)\n",
    "g_optimizer = torch.optim.Adam(G.parameters(), lr=0.0003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0/200], Step[300/600], d_loss: 1.5087, g_loss: 0.9395, D(x): 0.59, D(G(z)): 0.56\n",
      "Epoch [1/200], Step[300/600], d_loss: 0.3804, g_loss: 2.8466, D(x): 0.91, D(G(z)): 0.22\n",
      "Epoch [2/200], Step[300/600], d_loss: 0.5711, g_loss: 3.1011, D(x): 0.85, D(G(z)): 0.24\n",
      "Epoch [3/200], Step[300/600], d_loss: 0.6536, g_loss: 2.7380, D(x): 0.88, D(G(z)): 0.32\n",
      "Epoch [4/200], Step[300/600], d_loss: 0.6742, g_loss: 3.7023, D(x): 0.83, D(G(z)): 0.24\n",
      "Epoch [5/200], Step[300/600], d_loss: 0.6428, g_loss: 2.3512, D(x): 0.78, D(G(z)): 0.26\n",
      "Epoch [6/200], Step[300/600], d_loss: 0.3800, g_loss: 2.9065, D(x): 0.92, D(G(z)): 0.20\n",
      "Epoch [7/200], Step[300/600], d_loss: 0.9713, g_loss: 1.8957, D(x): 0.69, D(G(z)): 0.32\n",
      "Epoch [8/200], Step[300/600], d_loss: 2.0707, g_loss: 0.5655, D(x): 0.37, D(G(z)): 0.50\n",
      "Epoch [9/200], Step[300/600], d_loss: 1.8433, g_loss: 3.7206, D(x): 0.57, D(G(z)): 0.32\n",
      "Epoch [10/200], Step[300/600], d_loss: 2.7838, g_loss: 0.6028, D(x): 0.31, D(G(z)): 0.64\n",
      "Epoch [11/200], Step[300/600], d_loss: 0.9253, g_loss: 1.5758, D(x): 0.74, D(G(z)): 0.39\n",
      "Epoch [12/200], Step[300/600], d_loss: 0.7292, g_loss: 1.8174, D(x): 0.69, D(G(z)): 0.23\n",
      "Epoch [13/200], Step[300/600], d_loss: 3.7925, g_loss: 0.2541, D(x): 0.23, D(G(z)): 0.78\n",
      "Epoch [14/200], Step[300/600], d_loss: 2.4964, g_loss: 2.8627, D(x): 0.37, D(G(z)): 0.29\n",
      "Epoch [15/200], Step[300/600], d_loss: 1.2309, g_loss: 2.8181, D(x): 0.67, D(G(z)): 0.33\n",
      "Epoch [16/200], Step[300/600], d_loss: 0.9526, g_loss: 1.8924, D(x): 0.68, D(G(z)): 0.26\n",
      "Epoch [17/200], Step[300/600], d_loss: 0.8770, g_loss: 2.5240, D(x): 0.67, D(G(z)): 0.20\n",
      "Epoch [18/200], Step[300/600], d_loss: 0.6558, g_loss: 2.8291, D(x): 0.81, D(G(z)): 0.29\n",
      "Epoch [19/200], Step[300/600], d_loss: 1.7029, g_loss: 1.5918, D(x): 0.59, D(G(z)): 0.35\n",
      "Epoch [20/200], Step[300/600], d_loss: 1.1095, g_loss: 2.4915, D(x): 0.62, D(G(z)): 0.19\n",
      "Epoch [21/200], Step[300/600], d_loss: 0.7743, g_loss: 1.9117, D(x): 0.72, D(G(z)): 0.25\n",
      "Epoch [22/200], Step[300/600], d_loss: 0.8932, g_loss: 3.1765, D(x): 0.70, D(G(z)): 0.23\n",
      "Epoch [23/200], Step[300/600], d_loss: 0.8487, g_loss: 2.6934, D(x): 0.75, D(G(z)): 0.20\n",
      "Epoch [24/200], Step[300/600], d_loss: 0.6678, g_loss: 2.2843, D(x): 0.85, D(G(z)): 0.31\n",
      "Epoch [25/200], Step[300/600], d_loss: 0.6217, g_loss: 2.4175, D(x): 0.79, D(G(z)): 0.22\n",
      "Epoch [26/200], Step[300/600], d_loss: 0.9993, g_loss: 1.8236, D(x): 0.63, D(G(z)): 0.24\n",
      "Epoch [27/200], Step[300/600], d_loss: 0.5810, g_loss: 3.0003, D(x): 0.77, D(G(z)): 0.17\n",
      "Epoch [28/200], Step[300/600], d_loss: 0.9926, g_loss: 2.2419, D(x): 0.73, D(G(z)): 0.26\n",
      "Epoch [29/200], Step[300/600], d_loss: 0.8989, g_loss: 2.1438, D(x): 0.78, D(G(z)): 0.33\n",
      "Epoch [30/200], Step[300/600], d_loss: 0.5606, g_loss: 3.2233, D(x): 0.88, D(G(z)): 0.23\n",
      "Epoch [31/200], Step[300/600], d_loss: 0.6892, g_loss: 1.8986, D(x): 0.77, D(G(z)): 0.18\n",
      "Epoch [32/200], Step[300/600], d_loss: 1.0796, g_loss: 2.7012, D(x): 0.74, D(G(z)): 0.35\n",
      "Epoch [33/200], Step[300/600], d_loss: 0.8599, g_loss: 2.6565, D(x): 0.79, D(G(z)): 0.30\n",
      "Epoch [34/200], Step[300/600], d_loss: 0.7413, g_loss: 1.7849, D(x): 0.73, D(G(z)): 0.21\n",
      "Epoch [35/200], Step[300/600], d_loss: 0.9280, g_loss: 2.2185, D(x): 0.73, D(G(z)): 0.32\n",
      "Epoch [36/200], Step[300/600], d_loss: 0.8056, g_loss: 2.0787, D(x): 0.74, D(G(z)): 0.23\n",
      "Epoch [37/200], Step[300/600], d_loss: 1.1451, g_loss: 1.6379, D(x): 0.72, D(G(z)): 0.36\n",
      "Epoch [38/200], Step[300/600], d_loss: 0.8684, g_loss: 1.9980, D(x): 0.69, D(G(z)): 0.20\n",
      "Epoch [39/200], Step[300/600], d_loss: 1.0489, g_loss: 1.4580, D(x): 0.73, D(G(z)): 0.37\n",
      "Epoch [40/200], Step[300/600], d_loss: 0.9849, g_loss: 2.6337, D(x): 0.62, D(G(z)): 0.18\n",
      "Epoch [41/200], Step[300/600], d_loss: 1.0483, g_loss: 1.7040, D(x): 0.73, D(G(z)): 0.35\n",
      "Epoch [42/200], Step[300/600], d_loss: 0.6482, g_loss: 2.3493, D(x): 0.81, D(G(z)): 0.27\n",
      "Epoch [43/200], Step[300/600], d_loss: 1.5352, g_loss: 1.4983, D(x): 0.55, D(G(z)): 0.33\n",
      "Epoch [44/200], Step[300/600], d_loss: 0.7896, g_loss: 2.2890, D(x): 0.71, D(G(z)): 0.24\n",
      "Epoch [45/200], Step[300/600], d_loss: 1.0547, g_loss: 1.6757, D(x): 0.76, D(G(z)): 0.42\n",
      "Epoch [46/200], Step[300/600], d_loss: 1.0720, g_loss: 1.6046, D(x): 0.68, D(G(z)): 0.36\n",
      "Epoch [47/200], Step[300/600], d_loss: 0.8497, g_loss: 1.6916, D(x): 0.69, D(G(z)): 0.27\n",
      "Epoch [48/200], Step[300/600], d_loss: 1.3565, g_loss: 1.3239, D(x): 0.71, D(G(z)): 0.48\n",
      "Epoch [49/200], Step[300/600], d_loss: 0.8544, g_loss: 1.7203, D(x): 0.69, D(G(z)): 0.27\n",
      "Epoch [50/200], Step[300/600], d_loss: 0.8407, g_loss: 1.3155, D(x): 0.70, D(G(z)): 0.29\n",
      "Epoch [51/200], Step[300/600], d_loss: 0.8327, g_loss: 1.6188, D(x): 0.76, D(G(z)): 0.34\n",
      "Epoch [52/200], Step[300/600], d_loss: 1.1887, g_loss: 1.1821, D(x): 0.65, D(G(z)): 0.39\n",
      "Epoch [53/200], Step[300/600], d_loss: 0.8914, g_loss: 1.8220, D(x): 0.70, D(G(z)): 0.31\n",
      "Epoch [54/200], Step[300/600], d_loss: 1.2336, g_loss: 1.8675, D(x): 0.64, D(G(z)): 0.39\n",
      "Epoch [55/200], Step[300/600], d_loss: 0.8059, g_loss: 2.0678, D(x): 0.75, D(G(z)): 0.29\n",
      "Epoch [56/200], Step[300/600], d_loss: 1.0132, g_loss: 1.5715, D(x): 0.67, D(G(z)): 0.32\n",
      "Epoch [57/200], Step[300/600], d_loss: 0.8728, g_loss: 1.6282, D(x): 0.70, D(G(z)): 0.27\n",
      "Epoch [58/200], Step[300/600], d_loss: 0.8508, g_loss: 1.5168, D(x): 0.72, D(G(z)): 0.30\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "N_EPOCHS = 200\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    for i, (images, _) in enumerate(train_loader):\n",
    "        # Build mini-batch dataset\n",
    "        batch_size = images.size(0)\n",
    "        images = to_var(images.view(batch_size, -1))\n",
    "        \n",
    "        # Create the labels which are later used as input for the BCE loss\n",
    "        real_labels = to_var(torch.ones(batch_size))\n",
    "        fake_labels = to_var(torch.zeros(batch_size))\n",
    "\n",
    "        #============= Train the discriminator =============#\n",
    "        # k=1 (least expensive to train)        \n",
    "        # Compute BCE_Loss using real images where BCE_Loss(x, y): - y * log(D(x)) - (1-y) * log(1 - D(x))\n",
    "        # Second term of the loss is always zero since real_labels == 1\n",
    "        outputs = Dz(images).squeeze()\n",
    "        d_loss_real = criterion(outputs, real_labels)\n",
    "        real_score = outputs\n",
    "        \n",
    "        # Compute BCELoss using fake images\n",
    "        # First term of the loss is always zero since fake_labels == 0\n",
    "        z = to_var(torch.randn(batch_size, LATENT_SIZE))\n",
    "        fake_images = G(z)\n",
    "        outputs = Dz(fake_images).squeeze()\n",
    "        d_loss_fake = criterion(outputs, fake_labels)\n",
    "        fake_score = outputs\n",
    "        \n",
    "        # Backprop + Optimize\n",
    "        d_loss = d_loss_real + d_loss_fake\n",
    "        D.zero_grad()\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "        \n",
    "        #=============== Train the generator ===============#\n",
    "        # Compute loss with fake images\n",
    "        z = to_var(torch.randn(batch_size, LATENT_SIZE))\n",
    "        fake_images = G(z)\n",
    "        outputs = Dz(fake_images).squeeze()\n",
    "        \n",
    "        # We train G to maximize log(D(G(z)) instead of minimizing log(1-D(G(z)))\n",
    "        # For the reason, see the last paragraph of section 3. https://arxiv.org/pdf/1406.2661.pdf\n",
    "        g_loss = criterion(outputs, real_labels)\n",
    "        \n",
    "        # Backprop + Optimize\n",
    "        D.zero_grad()\n",
    "        G.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        \n",
    "        if (i+1) % 300 == 0:\n",
    "            print('Epoch [%d/%d], Step[%d/%d], d_loss: %.4f, '\n",
    "                  'g_loss: %.4f, D(x): %.2f, D(G(z)): %.2f' \n",
    "                  %(epoch, 200, i+1, 600, d_loss.data[0], g_loss.data[0],\n",
    "                    real_score.data.mean(), fake_score.data.mean()))\n",
    "    \n",
    "    # Save real images\n",
    "    if (epoch+1) == 1:\n",
    "        images = images.view(images.size(0), 1, 28, 28)\n",
    "        save_image(denorm(images.data), './data2/real_images.png')\n",
    "    \n",
    "    # Save sampled images\n",
    "    fake_images = fake_images.view(fake_images.size(0), 1, 28, 28)\n",
    "    save_image(denorm(fake_images.data), './data2/fake_images-%d.png' %(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "batch_size = 1\n",
    "N_INTERP_IMGS = 10\n",
    "\n",
    "for i in range(N_INTERP_IMGS):\n",
    "    z1 = to_var(torch.randn(batch_size, LATENT_SIZE))\n",
    "    z2 = to_var(torch.randn(batch_size, LATENT_SIZE))\n",
    "    fake_images1, fake_images2 = G(z1), G(z2)\n",
    "\n",
    "    alphas = np.range(0, 1.1, 0.1)\n",
    "    z = torch.stack([alpha * z1 + (1 - alpha) * z2 for alpha in alphas])\n",
    "    fake_images = G(z)\n",
    "\n",
    "    fake_images = fake_images.view(fake_images.size(0), 1, 28, 28)\n",
    "    save_image(denorm(fake_images.data), './data/interp_fake_images-%d.png' % (i + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great now we are ready to begin modeling. Performance-wise, you want tune your hyperparameters based on the **evidence lower bound (ELBO)**. Recall that the ELBO is given by:\n",
    "\n",
    "$$ELBO = \\mathbb{E}_{q(\\mathbf{z} ; \\lambda)} [\\log p(\\mathbf{x} \\,|\\,\\mathbf{z} ; \\theta)] - \\mathbb{KL}[q(\\mathbf{z};\\lambda) \\, \\Vert \\, p(\\mathbf{z})]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variational parameters are given by running the encoder over the input, i..e. $\\lambda = encoder(\\mathbf{x};\\phi)$. The generative model (i.e. decoder) is parameterized by $\\theta$. Since we are working with binarized digits, $\\log p(x \\, | \\, \\mathbf{z} ; \\theta)$ is given by:\n",
    "\n",
    "$$ \\log p(x \\, | \\, \\mathbf{z} ; \\theta) = \\sum_{i=1}^{784} \\log \\sigma(\\mathbf{h})_{i} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\mathbf{h}$ is the final layer of the generative model (i.e. 28*28 = 784 dimensionval vector), and $\\sigma(\\cdot)$ is the sigmoid function. \n",
    "\n",
    "For the baseline model in this assignment you will be using a spherical normal prior, i.e. $p(\\mathbf{z}) = \\mathcal{N}(\\mathbf{0}, \\mathbf{I})$. The variational family will also be normal, i.e. $q(\\mathbf{z} ; \\lambda) = \\mathcal{N}(\\boldsymbol{\\mu}, \\log \\boldsymbol \\sigma^2)$ (here we will work with normal families with diagonal covariance). The KL-divergence between the variational posterior $q(\\mathbf{z})$ and the prior $p(\\mathbf{z})$ has a closed-form analytic solution, which is available in the original VAE paper referenced above. (If you are using the torch distributions package they will automatically calculate it for you, however you will need to use pytorch 0.4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For GANs you should use the same data in its continuous form. Here use the same prior, but use a multi-layer network to map to a continous 28x28 output space. Then use a multilayer discriminator to classify. \n",
    "\n",
    "For both models you may also consider trying a deconvolutional network (as in DCGAN) to produce output from the latent variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to quantitative metrics (i.e. ELBO), we are also going to ask you to do some qualitative analysis via visualizations. Please include the following in your report:\n",
    "\n",
    "1. Generate a bunch of digits from your generative model (sample $\\mathbf{z} \\sim p(\\mathbf{z})$, then $\\mathbf{x} \\sim p (\\mathbf{x} \\, | \\, \\mathbf{z} ; \\theta$))\n",
    "2. Sample two random latent vectors $\\mathbf{z}_1, \\mathbf{z}_2 \\sim p(\\mathbf{z})$, then sample from their interpolated values, i.e. $\\mathbf{z} \\sim p (\\mathbf{x} \\, | \\, \\alpha\\mathbf{z}_1 + (1-\\alpha)\\mathbf{z}_2; \\theta$) for $\\alpha = \\{0, 0.2, 0.4, 0.6, 0.8 ,1.0 \\}$.\n",
    "3. Train a VAE with 2 latent dimensions. Make a scatter plot of the variational means, $\\mu_1, \\mu_2$, where the color\n",
    "corresponds to the digit.\n",
    "4. With the same model as in (3), pick a 2d grid around the origin (0,0), e.g. with\n",
    "`np.meshgrid(np.linspace(-2, 2, 10), np.linspace(-2, 2, 10)`. For each point in the grid $(z_1, z_2)$, generate\n",
    "$\\mathbf{x}$ and show the corresponding digit in the 2d plot. For an example see [here](http://fastforwardlabs.github.io/blog-images/miriam/tableau.1493x693.png) (the right image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [python3]",
   "language": "python",
   "name": "Python [python3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
